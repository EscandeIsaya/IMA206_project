{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtasiNet : \n",
    "\tdef __init__(self,A) -> None:\n",
    "\t\tself.A = A\n",
    "\t\treturn\n",
    "\n",
    "\tdef compute_W(self,max_iter=1000, tol=1e-6, alpha=0.01):\n",
    "\t\tA = self.A\n",
    "\t\tM, N = A.shape\n",
    "\t\tW = np.random.rand(M, N) + 1j * np.random.rand(M, N)  # Initial guess\n",
    "\t\tLambda = np.zeros((M, N), dtype=complex)  # Lagrange multipliers\n",
    "\n",
    "\t\tfor iteration in range(max_iter):\n",
    "\t\t\tW_prev = W.copy()\n",
    "\n",
    "\t\t\t# Update W\n",
    "\t\t\tfor i in range(N):\n",
    "\t\t\t\tA_i = A[:, i]\n",
    "\t\t\t\tLambda_i = Lambda[:, i]\n",
    "\n",
    "\t\t\t\t# Gradient of the objective function\n",
    "\t\t\t\tgrad = 2 * (W.T @ A) @ A[:, i] + Lambda_i\n",
    "\n",
    "\t\t\t\t# Update W column by column\n",
    "\t\t\t\tW[:, i] -= alpha * grad\n",
    "\t\t\t\t# Enforce the constraint\n",
    "\t\t\t\tW[:, i] = W[:, i] / (np.dot(W[:, i], A_i.conj()) + 1e-10)  # Normalization to ensure constraint\n",
    "\n",
    "\t\t\t# Update Lagrange multipliers\n",
    "\t\t\tfor i in range(N):\n",
    "\t\t\t\tLambda[:, i] += alpha * (np.dot(W[:, i].T, A[:, i]) - 1)\n",
    "\n",
    "\t\t\t# Check convergence\n",
    "\t\t\tif np.linalg.norm(W - W_prev) < tol:\n",
    "\t\t\t\tprint(f\"Convergence reached after {iteration} iterations.\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn W\n",
    "\n",
    "\n",
    "\tdef run_algorithm(self, y, A, mu, beta, K):\n",
    "\t\t# Initialize variables\n",
    "\t\tymax = np.max(np.abs(y))\n",
    "\t\tepsilon = 0.005 * ymax\n",
    "\t\tbeta_0 = 0.01\n",
    "\t\tgamma_0 = np.zeros(A.shape[1])\n",
    "\t\tD_0 = np.zeros_like(y)\n",
    "\n",
    "\t\t# Pre-compute W\n",
    "\t\tW = self.compute_W()\n",
    "\n",
    "\t\t# Initialize variables for the loop\n",
    "\t\tgamma_k = gamma_0\n",
    "\t\tD_k = D_0\n",
    "\t\tk = 0\n",
    "\n",
    "\t\tfor k in range(K):\n",
    "\t\t\t# Update Wk\n",
    "\t\t\tWk = beta[k] * W\n",
    "\t\t\t# Update zk\n",
    "\t\t\tz_k = gamma_k - Wk @ D_k\n",
    "\t\t\t# Update Dk\n",
    "\t\t\tD_k = A @ gamma_k - y\n",
    "\t\t\t# Update θk\n",
    "\t\t\ttheta_k = mu[k] / (np.abs(z_k) + epsilon)\n",
    "\t\t\t# Update γk+1\n",
    "\t\t\tgamma_k = self.eta_threshold(z_k, theta_k)\n",
    "\n",
    "\t\t# Final reflectivity profile\n",
    "\t\tgamma = gamma_k\n",
    "\t\treturn gamma, theta_k\n",
    "\t\n",
    "\tdef __call__(self, y, A, mu, beta, K):\n",
    "\t\treturn self.run_algorithm(y, A, mu, beta, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
